diff --git a/packages/server/battle-room-rl/Battle-Room-Environment/battle-room-enviroment/env/wandb_ppo_tut.py b/packages/server/battle-room-rl/Battle-Room-Environment/battle-room-enviroment/env/wandb_ppo_tut.py
index e552d87..3f80199 100644
--- a/packages/server/battle-room-rl/Battle-Room-Environment/battle-room-enviroment/env/wandb_ppo_tut.py
+++ b/packages/server/battle-room-rl/Battle-Room-Environment/battle-room-enviroment/env/wandb_ppo_tut.py
@@ -34,6 +34,8 @@ def parse_args():
     parser.add_argument('--ent-coef', type=float, default=0.01, help='entropy loss coefficient')
     parser.add_argument('--vf-coef', type=float, default=0.01, help='value loss function coefficient')
     parser.add_argument('--max-grad-norm', type=float, default=0.5, help='the maximum norm for the gradient clipping')
+    parser.add_argument('--target-kl', type=float, default=None, help='0.015 suggested, target KL divergence threshold, used for early stopping')
+
     args = parser.parse_args()
     args.batch_size = int(args.num_envs * args.num_steps)
     args.minibatch_size = int(args.batch_size // args.num_minibatches)
@@ -184,12 +186,12 @@ if __name__ == "__main__":
 
         # optimizing the policy and value networks
         b_inds = np.arange(args.batch_size)
+        clipfracs = []
         for epoch in range(args.update_epochs):
             np.random.shuffle(b_inds)
             for start in range(0, args.batch_size, args.minibatch_size):
                 end = start + args.minibatch_size
                 mb_inds = b_inds[start:end]
-                # print(start, end)
                 _, newlogprob, entropy, new_values = agent.get_action_and_value(
                         b_obs[mb_inds], b_actions.long()[mb_inds]
                         )
@@ -200,6 +202,14 @@ if __name__ == "__main__":
                 if args.norm_adv:
                     mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
 
+                # debug values
+                with torch.no_grad():
+                    # how agressively the policy updates
+                    approx_kl = ((ratio - 1) - logratio).mean()
+                    # how often clip is triggered
+                    clipfracs += [((ratio - 1.0).abs () > args.clip_coef).float().mean()]
+
+
                 # policy loss
                 pg_loss1 = -mb_advantages * ratio
                 pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
@@ -228,3 +238,25 @@ if __name__ == "__main__":
                 loss.backward()
                 nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
 
+            # early stopping, implemented at batch level but could be done at minibatch level
+            if args.target_kl is not None:
+                if approx_kl > args.target_kl: break
+
+        # explained variance - if value function is a good indicator of the returns
+        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
+        var_y = np.var(y_true)
+        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
+
+        # record rewards for plotting purposes
+        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
+        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
+        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
+        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
+        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
+        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
+        writer.add_scalar("losses/explained_variance", explained_var, global_step)
+        print("SPS:", int(global_step / (time.time() - start_time)))
+        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
+
+    envs.close()
+    writer.close()
